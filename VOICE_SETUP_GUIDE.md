# ElevenLabs Voice Call Setup Guide

# Voice Conversation Feature - FULLY IMPLEMENTED ‚úÖ

## Overview

Real-time voice conversation with an AI coding mentor is now **fully functional**!

The system uses:

- **Gemini AI** - For intelligent, conversational responses about code
- **ElevenLabs TTS** - For natural, human-like voice output
- **Web Speech API** - For browser-based speech recognition

## How to Use

### 1. Start a Voice Call

1. Navigate to "Talk to the Repo" section
2. Select up to 3 files from the file tree
3. Click the arrow button ‚Üí Select "Talk" mode
4. The AI will greet you and ask what you'd like to know

### 2. Have a Conversation

- **Tap the mic button** to start speaking
- Your speech is transcribed in real-time
- The AI responds with voice + text
- The AI asks follow-up questions to keep the conversation going
- Keep talking as long as you like!

### 3. End the Call

- Click the red phone button to end the call anytime
- Or switch to "Chat Mode" for text-based conversation

## Features

- üéôÔ∏è **Live Transcription** - See your words as you speak
- üîä **Natural Voice** - ElevenLabs provides human-like speech
- üí¨ **Conversation History** - Full transcript in chat bubbles
- üîá **Mute/Unmute** - Control your microphone
- üîà **Speaker Toggle** - Mute AI audio if needed
- ‚è±Ô∏è **Call Timer** - Track conversation duration

---

## Technical Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     FRONTEND (React/Next.js)                 ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  Web Speech API                    Audio Playback            ‚îÇ
‚îÇ  ‚îú‚îÄ SpeechRecognition ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ Transcript                 ‚îÇ
‚îÇ  ‚îî‚îÄ onresult event                                           ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ  User Speaks ‚Üí Transcript ‚Üí API Call ‚Üí AI Response ‚Üí Audio   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚îÇ
                              ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     BACKEND (Go/Gin)                         ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  POST /api/voice-chat                                        ‚îÇ
‚îÇ  ‚îú‚îÄ Receives: owner, repo, files, message, history           ‚îÇ
‚îÇ  ‚îú‚îÄ Fetches file contents from GitHub                        ‚îÇ
‚îÇ  ‚îú‚îÄ Calls Gemini with voice-optimized prompt                 ‚îÇ
‚îÇ  ‚îú‚îÄ Calls ElevenLabs TTS with response text                  ‚îÇ
‚îÇ  ‚îî‚îÄ Returns: { response: string, audio: base64 }             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚îÇ
              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
              ‚ñº                               ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Gemini AI API     ‚îÇ         ‚îÇ  ElevenLabs TTS     ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§         ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ - Voice-optimized   ‚îÇ         ‚îÇ - eleven_multilingual‚îÇ
‚îÇ   prompts           ‚îÇ         ‚îÇ   _v2 model         ‚îÇ
‚îÇ - Short, natural    ‚îÇ         ‚îÇ - George voice      ‚îÇ
‚îÇ   responses         ‚îÇ         ‚îÇ   (professional)    ‚îÇ
‚îÇ - Follow-up         ‚îÇ         ‚îÇ - Base64 audio      ‚îÇ
‚îÇ   questions         ‚îÇ         ‚îÇ   output            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## Setup Requirements

### Environment Variables

Add these to your `.env` file:

```bash
# Required for voice feature
ELEVENLABS_API_KEY=your_elevenlabs_api_key_here

# Already required
GEMINI_API_KEY=your_gemini_api_key_here
GITHUB_TOKEN=your_github_token_here
```

### Get Your ElevenLabs API Key

1. Go to https://elevenlabs.io
2. Sign up / Log in
3. Go to Profile Settings ‚Üí API Keys
4. Generate a new API key
5. Copy and add to `.env` file

---

## Backend Implementation

### Voice Chat Handler (`internal/introspect/handler.go`)

```go
// VoiceChatWithRepo handles voice conversations - returns text + audio
func (h *Handler) VoiceChatWithRepo(c *gin.Context) {
    var req VoiceChatRequest
    if err := c.ShouldBindJSON(&req); err != nil {
        c.JSON(http.StatusBadRequest, gin.H{"error": "..."})
        return
    }

    // Get AI response (shorter for voice)
    chatReq := &ai.ChatRequest{
        Owner:   req.Owner,
        Repo:    req.Repo,
        Files:   req.Files,
        Message: req.Message,
        History: req.History,
    }

    response, err := h.geminiClient.GenerateVoiceResponse(h.githubClient, chatReq)
    if err != nil {
        c.JSON(http.StatusInternalServerError, gin.H{"error": err.Error()})
        return
    }

    // Convert to speech
    audioData, err := h.elevenLabsClient.TextToSpeech(response.Response)
    if err != nil {
        // Return text response even if TTS fails
        c.JSON(http.StatusOK, gin.H{
            "response":    response.Response,
            "audio_error": err.Error(),
        })
        return
    }

    // Return both text and base64-encoded audio
    audioBase64 := base64.StdEncoding.EncodeToString(audioData)
    c.JSON(http.StatusOK, gin.H{
        "response": response.Response,
        "audio":    audioBase64,
    })
}
```

### Voice-Optimized Gemini Prompt (`internal/ai/chat.go`)

The `GenerateVoiceResponse` function uses a special prompt:

```go
contextBuilder.WriteString("You are a FRIENDLY AI coding mentor having a REAL-TIME VOICE CONVERSATION...\n")
contextBuilder.WriteString("CRITICAL VOICE CONVERSATION RULES:\n")
contextBuilder.WriteString("1. Keep responses SHORT - 2-3 sentences MAXIMUM\n")
contextBuilder.WriteString("2. Use NATURAL spoken language - like a phone call with a friend\n")
contextBuilder.WriteString("3. Be WARM, encouraging, and supportive\n")
contextBuilder.WriteString("4. NEVER use code blocks, bullet points, or markdown\n")
contextBuilder.WriteString("5. ALWAYS end with a follow-up question\n")
// ... etc
```

---

## Frontend Implementation

### Key Components in `TalkToRepo.tsx`

1. **Speech Recognition Setup**

```tsx
const initSpeechRecognition = useCallback(() => {
  const SpeechRecognition =
    window.SpeechRecognition || window.webkitSpeechRecognition;
  const recognition = new SpeechRecognition();
  recognition.continuous = false;
  recognition.interimResults = true;
  recognition.lang = "en-US";
  // ... event handlers
  return recognition;
}, []);
```

2. **Voice Conversation State**

```tsx
const [voiceHistory, setVoiceHistory] = useState<ChatMessage[]>([]);
const [isAISpeaking, setIsAISpeaking] = useState(false);
const [isListening, setIsListening] = useState(false);
const [currentTranscript, setCurrentTranscript] = useState("");
const [voiceStatus, setVoiceStatus] = useState("Ready to start");
```

3. **Audio Playback**

```tsx
<audio ref={audioRef} className="hidden" />;

// In handleUserSpeech:
const audioSrc = `data:audio/mpeg;base64,${response.audio}`;
audioRef.current.src = audioSrc;
audioRef.current.onended = () => {
  setIsAISpeaking(false);
  // Auto-start listening after AI finishes
  setTimeout(() => startListening(), 500);
};
await audioRef.current.play();
```

---

## Browser Compatibility

| Browser | Speech Recognition | Audio Playback  |
| ------- | ------------------ | --------------- |
| Chrome  | ‚úÖ Full support    | ‚úÖ Full support |
| Edge    | ‚úÖ Full support    | ‚úÖ Full support |
| Firefox | ‚ùå Limited         | ‚úÖ Full support |
| Safari  | ‚ö†Ô∏è webkit prefix   | ‚úÖ Full support |

**Note:** For the best experience, use Chrome or Edge.

---

## Troubleshooting

### "Speech recognition not supported"

- Use Chrome or Edge browser
- Make sure you're on HTTPS (required for speech recognition)

### No audio playing

- Check that speaker is not muted (speaker button)
- Check browser's audio permissions
- Verify ELEVENLABS_API_KEY is set correctly

### AI not responding

- Check backend logs for errors
- Verify GEMINI_API_KEY is set correctly
- Ensure files are selected before starting call

### Microphone not working

- Allow microphone access when browser prompts
- Check that mic is not muted (mic button)
- Try refreshing the page

---

## Customization

### Change AI Voice

In `internal/ai/elevenlabs.go`, change the `defaultVoiceID`:

```go
const (
    // Available voices:
    // "JBFqnCBsd6RMkjVDRZzb" - George (professional male)
    // "21m00Tcm4TlvDq8ikWAM" - Rachel (natural female)
    // "EXAVITQu4vr4xnSDxMaL" - Bella (soft female)
    // "ErXwobaYiN019PkySvjV" - Antoni (warm male)
    defaultVoiceID = "JBFqnCBsd6RMkjVDRZzb"
)
```

### Adjust Response Length

In `internal/ai/chat.go`, modify the `GenerationConfig`:

```go
GenerationConfig: GenerationConfig{
    Temperature:     0.7,
    MaxOutputTokens: 200, // Shorter for voice
},
```
